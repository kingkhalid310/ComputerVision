{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing pipeline for the QSM 3D segmentation project \n",
        "# Course: cs512\n",
        "# instructor: Gady Agam\n",
        "# Group 1: \n",
        "# Rasheed Abid, rabid@hawk.iit.edu\n",
        "# Khalid Saifullah, ksaifullah@hawk.iit.edu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: patchify in /Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages (from patchify) (1.20.3)\n"
          ]
        }
      ],
      "source": [
        "# installations if required\n",
        "!pip install patchify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1AlJ_fpd8scE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version = 2.11.1\n",
            "Keras version = 2.11.0\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from patchify import patchify, unpatchify\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Conv3DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Activation, MaxPool2D, Concatenate\n",
        "\n",
        "print(\"Tensorflow version = \" + tf.__version__)\n",
        "print(\"Keras version = \" + keras.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6L3eZwk86Yr",
        "outputId": "7a68505a-8b13-4678-a973-5d12a6c38f07"
      },
      "outputs": [],
      "source": [
        "# # incase of running on google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# changing variables\n",
        "\n",
        "# maindir = \"/content/drive/MyDrive/Colab Notebooks/CS512/project\"\n",
        "maindir = \"/Users/rashid_abid/Desktop/CS512_Computer_Vision/Project_proposal/cs512_project_gdrive_runs\"\n",
        "subject_num = 80 # available ones are 32, 64, 80, 100\n",
        "subject_num_testing = 9 # available ones are 9, 20\n",
        "label_name = \"hippocampus\" # available ones are \"caudate\", \"putamen\", \"hippocampus\", \"thalamus\", \"amygdala\"\n",
        "epochs = 50\n",
        "dim = 128 # available ones are 64, 128, 256\n",
        "n_classes = 1\n",
        "\n",
        "# architecture parameters\n",
        "patch_size = 64 # testing with 64x64x64 patches\n",
        "channels=1\n",
        "\n",
        "LR = 0.0001 # Learning rate\n",
        "optim = keras.optimizers.Adam(LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# directories \n",
        "\n",
        "os.chdir( maindir )\n",
        "source_dir = os.path.join(maindir, \"data_dir\")\n",
        "results_dir = os.path.join(maindir, \"results_dir\")\n",
        "files_dir = os.path.join(maindir, \"files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "je1WviY89M2I",
        "outputId": "c0b10708-364d-4ef3-ccbd-bc0e09d3d98e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/rashid_abid/Desktop/CS512_Computer_Vision/Project_proposal/cs512_project_gdrive_runs/results_dir/saved_models/hippocampus_mask_dim128_sub80_e50.h5\n"
          ]
        }
      ],
      "source": [
        "# file names\n",
        "saved_model_name = label_name + \"_mask_dim\" + str(dim) + \"_sub\" + str(subject_num) + \"_e\" + str(epochs) + \".h5\"\n",
        "saved_model_name = os.path.join(results_dir, \"saved_models\", saved_model_name)\n",
        "\n",
        "subject_filename = \"filename_sub\" + str(subject_num) + \".txt\"\n",
        "filename = os.path.join(files_dir, subject_filename)\n",
        "test_filename = os.path.join(files_dir, \"filename_testing\" + str(subject_num_testing) + \".txt\")\n",
        "\n",
        "\n",
        "print(saved_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suffix for the files is = 4\n"
          ]
        }
      ],
      "source": [
        "# suffix selection for images\n",
        "if dim == 256:\n",
        "    Suffix = 2\n",
        "elif dim ==  128:\n",
        "    Suffix = 4\n",
        "else:\n",
        "    Suffix = 8\n",
        "\n",
        "print( \"Suffix for the files is = %d\" %Suffix )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Make sure the GPU is available. \n",
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to normalize the nifti data once converted to numpy array, and then multiply by the mask\n",
        "# of the brain\n",
        "\n",
        "def norm_nifti(img, brain_mask_data):\n",
        "    # We are normalizing the data to be between 0 and 1\n",
        "    min_value = np.min(img)\n",
        "    max_value = np.max(img)\n",
        "    normalized_data = (img - min_value) / (max_value - min_value)\n",
        "    \n",
        "    # We are multiplying the normalized data by the brain mask\n",
        "    # to make sure that the background is 0\n",
        "    normalized_data = normalized_data * brain_mask_data\n",
        "    \n",
        "    return normalized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss functions and dice metrics\n",
        "# both for binary and multiclass segmentation\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    smoothing_factor = 1 # to avoid division by zero\n",
        "    \n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    union = K.sum(y_true_f) + K.sum(y_pred_f)\n",
        "    dice = (2. * intersection + smoothing_factor) / (union + smoothing_factor)\n",
        "    return dice\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def dice_coef_multilabel(y_true, y_pred, numLabels=1):\n",
        "    dice=0\n",
        "    for index in range(numLabels):\n",
        "        dice -= dice_coef(y_true[...,index], y_pred[...,index])\n",
        "    return dice/numLabels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model from the saved file: /Users/rashid_abid/Desktop/CS512_Computer_Vision/Project_proposal/cs512_project_gdrive_runs/results_dir/saved_models/hippocampus_mask_dim128_sub80_e50.h5\n"
          ]
        }
      ],
      "source": [
        "# print the name of the model to check\n",
        "print(\"Loading the model from the saved file: \" + saved_model_name)\n",
        "\n",
        "# load the model\n",
        "model = load_model(saved_model_name, custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef})                  \n",
        "# model = keras.models.load_model(saved_model_name) # this one does not work for custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing phase with unseen dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The test dataset is being processed now....\n",
            "Currently processing subject number: 0\n",
            "The average dice score for the current subject is: 0.7193836\n",
            "Currently processing subject number: 1\n",
            "The average dice score for the current subject is: 0.8493917\n",
            "Currently processing subject number: 2\n",
            "The average dice score for the current subject is: 0.85301983\n",
            "Currently processing subject number: 3\n",
            "The average dice score for the current subject is: 0.835168\n",
            "Currently processing subject number: 4\n",
            "The average dice score for the current subject is: 0.604786\n",
            "Currently processing subject number: 5\n",
            "The average dice score for the current subject is: 0.8254431\n",
            "Currently processing subject number: 6\n",
            "The average dice score for the current subject is: 0.81360793\n",
            "Currently processing subject number: 7\n",
            "The average dice score for the current subject is: 0.87856996\n",
            "Currently processing subject number: 8\n",
            "The average dice score for the current subject is: 0.7315997\n"
          ]
        }
      ],
      "source": [
        "test_filename = test_filename\n",
        "reconstructed_dir = os.path.join( results_dir, \"reconstructed_predictions\" )\n",
        "# check if the directory exists, if not create it\n",
        "if not os.path.exists(reconstructed_dir):\n",
        "    os.makedirs(reconstructed_dir)\n",
        "\n",
        "all_img_patches = []\n",
        "all_mask_patches = []\n",
        "all_dice_scores = []\n",
        "val=0\n",
        "\n",
        "print(\"The test dataset is being processed now....\")\n",
        "with open(test_filename) as f:\n",
        "    for line in f:\n",
        "        print(\"Currently processing subject number: %d\" %val)\n",
        "        val=val+1\n",
        "        subject_id = line.strip()\n",
        "\n",
        "        # Read file names\n",
        "        imagename =  os.path.join(source_dir, subject_id, \"QSM_masked_dim\" + str(dim) + \".nii.gz\") \n",
        "        maskname = os.path.join(source_dir, subject_id, label_name + \"_dim\" + str(dim) + \".nii.gz\") \n",
        "        brain_mask_name = os.path.join(source_dir, subject_id, \"QSM_brain_mask_dim\" + str(dim) + \".nii.gz\") \n",
        "\n",
        "        # Read images\n",
        "        img = nib.load(imagename)\n",
        "        img_data = img.get_fdata().astype(np.float32)\n",
        "        \n",
        "        # Read mask\n",
        "        mask = nib.load(maskname)\n",
        "        mask_data = mask.get_fdata().astype(np.float32)\n",
        "\n",
        "        # Read brain mask\n",
        "        brain_mask = nib.load(brain_mask_name)\n",
        "        brain_mask_data = brain_mask.get_fdata().astype(np.float32)\n",
        "        \n",
        "        #Normalise using the function we wrote\n",
        "        img_data = norm_nifti(img_data, brain_mask_data)\n",
        "        \n",
        "        #Patchify the image and mask\n",
        "        patches_img = patchify(img_data, (patch_size, patch_size, patch_size), step=64)  # Step=256 for 256 patches means no overlap\n",
        "        patches_mask = patchify(mask_data, (patch_size, patch_size, patch_size), step=64)  # Step=256 for 256 patches means no overlap\n",
        "        \n",
        "\n",
        "        # get a temporary block to store the predicted patches for the current subject\n",
        "        temporary_blocks = np.empty(patches_img.shape)\n",
        "\n",
        "        dice_scores = []\n",
        "        for i in range(patches_img.shape[0]):\n",
        "            for j in range(patches_img.shape[1]):\n",
        "                for k in range(patches_img.shape[2]):\n",
        "                    \n",
        "                    #image\n",
        "                    single_patch_img = patches_img[i,j,k,:,:,:]\n",
        "                    all_img_patches.append(single_patch_img)\n",
        "                    \n",
        "                    #mask\n",
        "                    single_patch_mask = patches_mask[i,j,k,:,:,:]\n",
        "                    all_mask_patches.append(single_patch_mask)\n",
        "\n",
        "                    ### predicting on the patches\n",
        "                    temp_img_patch = np.expand_dims(single_patch_img, -1) # to fit the model input\n",
        "                    temp_img_patch = np.expand_dims(temp_img_patch, 0) # to fit the model input\n",
        "                    current_predict = model.predict(temp_img_patch, verbose=0) # predict\n",
        "                    current_predict = (current_predict>0.5).astype(np.float32)\n",
        "                    # get the dice score and save it in a list\n",
        "                    dice_score = dice_coef(single_patch_mask, current_predict)\n",
        "                    dice_scores.append(dice_score)\n",
        "\n",
        "                    temporary_blocks[i,j,k,...] = np.squeeze(current_predict) # save the prediction in the temporary block\n",
        "\n",
        "        \n",
        "        # print the average dice score for the current subject\n",
        "        print(\"The average dice score for the current subject is: \" + str(np.mean(dice_scores)))\n",
        "        # save the mean dice scores for all subjects in a list\n",
        "        all_dice_scores.append(np.mean(dice_scores))\n",
        "\n",
        "        # save the predicted patches for the current subject in the reconstructed images in the results folder\n",
        "        reconstructed_image = unpatchify(temporary_blocks, img_data.shape)\n",
        "        reconstructed_imges = nib.Nifti1Image(reconstructed_image, img.affine, img.header)\n",
        "\n",
        "        # check first, and then make a directory for the subject in the reconstructed folder, if it does not exist\n",
        "        if not os.path.exists(os.path.join(reconstructed_dir, str(subject_id))):\n",
        "            os.mkdir(os.path.join(reconstructed_dir, str(subject_id)))\n",
        "\n",
        "        recostructed_filename = os.path.join(reconstructed_dir, str(subject_id), label_name + \"_dim\" + str(dim) + \"_e\" + str(epochs) +  \"_recon_by_\" + str(Suffix) + \".nii.gz\")\n",
        "        nib.save(reconstructed_imges, recostructed_filename)\n",
        "        ## reconstruction is done for the current subject\n",
        "        \n",
        "\n",
        "        # image stacking \n",
        "        all_patched_stacked_images = np.array(all_img_patches)\n",
        "        # mask stacking\n",
        "        all_patched_stacked_masks = np.array(all_mask_patches)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average dice score for all 9 subjects is: 0.7901078\n"
          ]
        }
      ],
      "source": [
        "# print the average dice score for all subjects\n",
        "print(\"The average dice score for all \" + str(subject_num_testing) +  \" subjects is: \" + str(np.mean(all_dice_scores)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
