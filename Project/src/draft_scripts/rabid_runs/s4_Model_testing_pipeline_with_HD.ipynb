{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing pipeline for the QSM 3D segmentation project \n",
        "# Course: cs512\n",
        "# instructor: Gady Agam\n",
        "# Group 1: \n",
        "# Rasheed Abid, rabid@hawk.iit.edu\n",
        "# Khalid Saifullah, ksaifullah@hawk.iit.edu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: patchify in /Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages (from patchify) (1.20.3)\n"
          ]
        }
      ],
      "source": [
        "# installations if required\n",
        "!pip install patchify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1AlJ_fpd8scE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version = 2.11.1\n",
            "Keras version = 2.11.0\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from scipy.spatial.distance import directed_hausdorff\n",
        "\n",
        "from patchify import patchify, unpatchify\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Conv3DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Activation, MaxPool2D, Concatenate\n",
        "\n",
        "print(\"Tensorflow version = \" + tf.__version__)\n",
        "print(\"Keras version = \" + keras.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6L3eZwk86Yr",
        "outputId": "7a68505a-8b13-4678-a973-5d12a6c38f07"
      },
      "outputs": [],
      "source": [
        "# # incase of running on google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# changing variables\n",
        "\n",
        "# maindir = \"/content/drive/MyDrive/Colab Notebooks/CS512/project\"\n",
        "maindir = \"/Users/rashid_abid/Desktop/CS512_Computer_Vision/Project_proposal/cs512_project_gdrive_runs\"\n",
        "subject_num = 80 # available ones are 32, 64, 80, 100\n",
        "subject_num_testing = 20 # available ones are 9, 20\n",
        "label_name = \"hippocampus\" # available ones are \"caudate\", \"putamen\", \"hippocampus\", \"thalamus\", \"amygdala\"\n",
        "epochs = 50\n",
        "dim = 128 # available ones are 64, 128, 256\n",
        "n_classes = 1\n",
        "\n",
        "# architecture parameters\n",
        "patch_size = 64 # testing with 64x64x64 patches\n",
        "channels=1\n",
        "\n",
        "LR = 0.0001 # Learning rate\n",
        "optim = keras.optimizers.Adam(LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# directories \n",
        "\n",
        "os.chdir( maindir )\n",
        "source_dir = os.path.join(maindir, \"data_dir\")\n",
        "results_dir = os.path.join(maindir, \"results_dir\")\n",
        "files_dir = os.path.join(maindir, \"files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "je1WviY89M2I",
        "outputId": "c0b10708-364d-4ef3-ccbd-bc0e09d3d98e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/rashid_abid/Desktop/CS512_Computer_Vision/Project_proposal/cs512_project_gdrive_runs/results_dir/saved_models/hippocampus_mask_dim128_sub80_e50.h5\n"
          ]
        }
      ],
      "source": [
        "# file names\n",
        "saved_model_name = label_name + \"_mask_dim\" + str(dim) + \"_sub\" + str(subject_num) + \"_e\" + str(epochs) + \".h5\"\n",
        "saved_model_name = os.path.join(results_dir, \"saved_models\", saved_model_name)\n",
        "\n",
        "subject_filename = \"filename_sub\" + str(subject_num) + \".txt\"\n",
        "filename = os.path.join(files_dir, subject_filename)\n",
        "test_filename = os.path.join(files_dir, \"filename_testing\" + str(subject_num_testing) + \".txt\")\n",
        "\n",
        "\n",
        "print(saved_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suffix for the files is = 4\n"
          ]
        }
      ],
      "source": [
        "# suffix selection for images\n",
        "if dim == 256:\n",
        "    Suffix = 2\n",
        "elif dim ==  128:\n",
        "    Suffix = 4\n",
        "else:\n",
        "    Suffix = 8\n",
        "\n",
        "print( \"Suffix for the files is = %d\" %Suffix )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Make sure the GPU is available. \n",
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to normalize the nifti data once converted to numpy array, and then multiply by the mask\n",
        "# of the brain\n",
        "\n",
        "def norm_nifti(img, brain_mask_data):\n",
        "    # We are normalizing the data to be between 0 and 1\n",
        "    min_value = np.min(img)\n",
        "    max_value = np.max(img)\n",
        "    normalized_data = (img - min_value) / (max_value - min_value)\n",
        "    \n",
        "    # We are multiplying the normalized data by the brain mask\n",
        "    # to make sure that the background is 0\n",
        "    normalized_data = normalized_data * brain_mask_data\n",
        "    \n",
        "    return normalized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss functions and dice metrics\n",
        "# both for binary and multiclass segmentation\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    smoothing_factor = 1 # to avoid division by zero\n",
        "    \n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    union = K.sum(y_true_f) + K.sum(y_pred_f)\n",
        "    dice = (2. * intersection + smoothing_factor) / (union + smoothing_factor)\n",
        "    return dice\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def dice_coef_multilabel(y_true, y_pred, numLabels=1):\n",
        "    dice=0\n",
        "    for index in range(numLabels):\n",
        "        dice -= dice_coef(y_true[...,index], y_pred[...,index])\n",
        "    return dice/numLabels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model from the saved file: /Users/rashid_abid/Desktop/CS512_Computer_Vision/Project_proposal/cs512_project_gdrive_runs/results_dir/saved_models/hippocampus_mask_dim128_sub80_e50.h5\n"
          ]
        }
      ],
      "source": [
        "# print the name of the model to check\n",
        "print(\"Loading the model from the saved file: \" + saved_model_name)\n",
        "\n",
        "# load the model\n",
        "model = load_model(saved_model_name, custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef})                  \n",
        "# model = keras.models.load_model(saved_model_name) # this one does not work for custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing phase with unseen dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_hausdorff(y_ground, y_pred):\n",
        "    # make sure the shapes are the same, make them squeeze if required\n",
        "    y_ground = np.squeeze(y_ground)\n",
        "    y_pred = np.squeeze(y_pred)\n",
        "\n",
        "    assert y_ground.shape == y_pred.shape, \"The shapes of the ground truth and the prediction are not the same\"\n",
        "\n",
        "    # Find the indices of non-zero values in each mask\n",
        "    indices1 = np.argwhere(y_ground)\n",
        "    indices2 = np.argwhere(y_pred)\n",
        "\n",
        "    # Calculate the directed Hausdorff distance between the masks\n",
        "    max_distance = -math.inf # initialize the max distance to -infinity\n",
        "    for index1 in indices1:\n",
        "        min_distance = math.inf\n",
        "        for index2 in indices2:\n",
        "            distance = math.sqrt((index1[0] - index2[0])**2 + (index1[1] - index2[1])**2 + (index1[2] - index2[2])**2)\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "        if min_distance > max_distance:\n",
        "            max_distance = min_distance\n",
        "\n",
        "    # calculate the hausdorff distance\n",
        "    hausdorff_distance = max(max_distance, 0)\n",
        "    return hausdorff_distance\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The test dataset is being processed now....\n",
            "Currently processing subject number: 0\n",
            "(1, 64, 64, 64, 1)\n",
            "(1, 1, 64, 64, 64, 1)\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node 'Res-UNet/conv3d/Conv3D' defined at (most recent call last):\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/rx/ykb8mrxs0gldmbrd3wd9pr340000gn/T/ipykernel_4875/3732028610.py\", line 68, in <module>\n      current_predict = model.predict(temp_img_patch, verbose=0) # predict\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'Res-UNet/conv3d/Conv3D'\ninput must be 5-dimensional\n\t [[{{node Res-UNet/conv3d/Conv3D}}]] [Op:__inference_predict_function_6099]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/rx/ykb8mrxs0gldmbrd3wd9pr340000gn/T/ipykernel_4875/3732028610.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_img_patch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                     \u001b[0mcurrent_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_img_patch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                     \u001b[0mcurrent_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_predict\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;31m# get the dice score and save it in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'Res-UNet/conv3d/Conv3D' defined at (most recent call last):\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/rx/ykb8mrxs0gldmbrd3wd9pr340000gn/T/ipykernel_4875/3732028610.py\", line 68, in <module>\n      current_predict = model.predict(temp_img_patch, verbose=0) # predict\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/Users/rashid_abid/opt/anaconda3/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'Res-UNet/conv3d/Conv3D'\ninput must be 5-dimensional\n\t [[{{node Res-UNet/conv3d/Conv3D}}]] [Op:__inference_predict_function_6099]"
          ]
        }
      ],
      "source": [
        "test_filename = test_filename\n",
        "reconstructed_dir = os.path.join( results_dir, \"reconstructed_predictions\" )\n",
        "# check if the directory exists, if not create it\n",
        "if not os.path.exists(reconstructed_dir):\n",
        "    os.makedirs(reconstructed_dir)\n",
        "\n",
        "all_img_patches = []\n",
        "all_mask_patches = []\n",
        "all_dice_scores = []\n",
        "all_hd_scores = []\n",
        "val=0\n",
        "\n",
        "print(\"The test dataset is being processed now....\")\n",
        "with open(test_filename) as f:\n",
        "    for line in f:\n",
        "        print(\"Currently processing subject number: %d\" %val)\n",
        "        val=val+1\n",
        "        subject_id = line.strip()\n",
        "\n",
        "        # Read file names\n",
        "        imagename =  os.path.join(source_dir, subject_id, \"QSM_masked_dim\" + str(dim) + \".nii.gz\") \n",
        "        maskname = os.path.join(source_dir, subject_id, label_name + \"_dim\" + str(dim) + \".nii.gz\") \n",
        "        brain_mask_name = os.path.join(source_dir, subject_id, \"QSM_brain_mask_dim\" + str(dim) + \".nii.gz\") \n",
        "        # maskname = brain_mask_name\n",
        "\n",
        "        # Read images\n",
        "        img = nib.load(imagename)\n",
        "        img_data = img.get_fdata().astype(np.float32)\n",
        "        \n",
        "        # Read mask\n",
        "        mask = nib.load(maskname)\n",
        "        mask_data = mask.get_fdata().astype(np.float32)\n",
        "\n",
        "        # Read brain mask\n",
        "        brain_mask = nib.load(brain_mask_name)\n",
        "        brain_mask_data = brain_mask.get_fdata().astype(np.float32)\n",
        "        \n",
        "        #Normalise using the function we wrote\n",
        "        img_data = norm_nifti(img_data, brain_mask_data)\n",
        "        \n",
        "        #Patchify the image and mask\n",
        "        patches_img = patchify(img_data, (patch_size, patch_size, patch_size), step=64)  # Step=256 for 256 patches means no overlap\n",
        "        patches_mask = patchify(mask_data, (patch_size, patch_size, patch_size), step=64)  # Step=256 for 256 patches means no overlap\n",
        "        \n",
        "\n",
        "        # get a temporary block to store the predicted patches for the current subject\n",
        "        temporary_blocks = np.empty(patches_img.shape)\n",
        "\n",
        "        dice_scores = []\n",
        "        hd_scores = []\n",
        "        for i in range(patches_img.shape[0]):\n",
        "            for j in range(patches_img.shape[1]):\n",
        "                for k in range(patches_img.shape[2]):\n",
        "                    \n",
        "                    #image\n",
        "                    single_patch_img = patches_img[i,j,k,:,:,:]\n",
        "                    all_img_patches.append(single_patch_img)\n",
        "                    \n",
        "                    #mask\n",
        "                    single_patch_mask = patches_mask[i,j,k,:,:,:]\n",
        "                    all_mask_patches.append(single_patch_mask)\n",
        "\n",
        "                    ### predicting on the patches\n",
        "                    temp_img_patch = np.expand_dims(single_patch_img, -1) # to fit the model input\n",
        "                    temp_img_patch = np.expand_dims(temp_img_patch, 0) # to fit the model input\n",
        "                    print(temp_img_patch.shape)\n",
        "\n",
        "                    current_predict = model.predict(temp_img_patch, verbose=0) # predict\n",
        "                    current_predict = (current_predict>0.5).astype(np.float32)\n",
        "                    # get the dice score and save it in a list\n",
        "                    dice_score = dice_coef(single_patch_mask, current_predict)\n",
        "                    dice_scores.append(dice_score)\n",
        "\n",
        "                    # get the hausdorff distance score and save it in a list\n",
        "                    hd_score = calculate_hausdorff(single_patch_mask, current_predict)\n",
        "                    hd_scores.append(hd_score)\n",
        "\n",
        "                    temporary_blocks[i,j,k,...] = np.squeeze(current_predict) # save the prediction in the temporary block\n",
        "\n",
        "        \n",
        "        # print the average dice score for the current subject\n",
        "        print(\"The average dice score for the current subject is: \" + str(np.mean(dice_scores)))\n",
        "        # save the mean dice scores for all subjects in a list\n",
        "        all_dice_scores.append(np.mean(dice_scores))\n",
        "\n",
        "        # print the average hausdorff distance score for the current subject\n",
        "        print(\"The average hausdorff distance score for the current subject is: \" + str(np.mean(hd_scores)))\n",
        "        # save the mean hausdorff distance scores for all subjects in a list\n",
        "        all_hd_scores.append(np.mean(hd_scores))\n",
        "\n",
        "        # save the predicted patches for the current subject in the reconstructed images in the results folder\n",
        "        reconstructed_image = unpatchify(temporary_blocks, img_data.shape)\n",
        "        reconstructed_imges = nib.Nifti1Image(reconstructed_image, img.affine, img.header)\n",
        "\n",
        "        # check first, and then make a directory for the subject in the reconstructed folder, if it does not exist\n",
        "        if not os.path.exists(os.path.join(reconstructed_dir, str(subject_id))):\n",
        "            os.mkdir(os.path.join(reconstructed_dir, str(subject_id)))\n",
        "\n",
        "        recostructed_filename = os.path.join(reconstructed_dir, str(subject_id), label_name + \"_dim\" + str(dim) + \"_e\" + str(epochs) +  \"_recon_by_\" + str(Suffix) + \".nii.gz\")\n",
        "        nib.save(reconstructed_imges, recostructed_filename)\n",
        "        ## reconstruction is done for the current subject\n",
        "        \n",
        "\n",
        "        # image stacking \n",
        "        all_patched_stacked_images = np.array(all_img_patches)\n",
        "        # mask stacking\n",
        "        all_patched_stacked_masks = np.array(all_mask_patches)\n",
        "\n",
        "        print() # for a new line\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average dice score for all 20 subjects is: 0.94765234\n"
          ]
        }
      ],
      "source": [
        "# print the average dice score for all subjects\n",
        "print(\"The average dice score for all \" + str(subject_num_testing) +  \" subjects is: \" + str(np.mean(all_dice_scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average hausdorff distance score for all 20 subjects is: 0.5084957114562725\n"
          ]
        }
      ],
      "source": [
        "# print the average hausdorff distance score for all subjects\n",
        "print(\"The average hausdorff distance score for all \" + str(subject_num_testing) +  \" subjects is: \" + str(np.mean(all_hd_scores)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
